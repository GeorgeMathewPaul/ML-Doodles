{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.\tConsider PlayTennis Dataset (see attached csv file) that is used for predicting whether a tennis game is played in the given weather conditions or not. Here the weather conditions are described by features outlook, temperature, humidity, play and wind. The target is play with two class labels Yes and No. \n",
        "\n",
        "i.\tCompute information gain for all the attributes and display them.\n"
      ],
      "metadata": {
        "id": "IYwRt3PP-wNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii.\tFind which attribute will become the root node of the decision tree."
      ],
      "metadata": {
        "id": "AZDvIyM9_-y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Load the PlayTennis dataset\n",
        "df = pd.read_csv('play (1).csv')\n",
        "\n",
        "# Define a function to calculate entropy\n",
        "def entropy(target_col):\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    entropy_val = 0\n",
        "    for i in range(len(elements)):\n",
        "        prob = counts[i]/np.sum(counts)\n",
        "        entropy_val += -prob * math.log2(prob)\n",
        "    return entropy_val\n",
        "\n",
        "# Define a function to calculate information gain\n",
        "def info_gain(data, split_attribute_name, target_name=\"play\"):\n",
        "    # Calculate the total entropy\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    # Calculate the weighted entropy of the subgroups after splitting on the chosen attribute\n",
        "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "    weighted_entropy = 0\n",
        "    for i in range(len(vals)):\n",
        "        prob = counts[i] / np.sum(counts)\n",
        "        subset = data[data[split_attribute_name] == vals[i]]\n",
        "        subset_entropy = entropy(subset[target_name])\n",
        "        weighted_entropy += prob * subset_entropy\n",
        "    \n",
        "    # Calculate the information gain\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "    return info_gain\n",
        "\n",
        "# Find the information gain for each feature and choose the one with the highest value as the root node\n",
        "features = df.columns[:-1]\n",
        "target = df.columns[-1]\n",
        "root_node = None\n",
        "max_info_gain = 0\n",
        "for feature in features:\n",
        "    cur_info_gain = info_gain(df, feature, target)\n",
        "    if cur_info_gain > max_info_gain:\n",
        "        max_info_gain = cur_info_gain\n",
        "        root_node = feature\n",
        "\n",
        "print(\"The root node of the decision tree is:\", root_node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlKQcb0rACA4",
        "outputId": "ba2526e7-25e6-4fa3-a696-a968766bb8b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The root node of the decision tree is: day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii.\tScikit DecisionTreeClassifier\n",
        "\n",
        "              •\tTrain using DecisionTreeClassifier using tennis dataset;\n",
        "\n",
        "              •\tClassify the test sample  <Rain, Cool, High,Weak>. \n",
        "\n",
        "              •\tDraw the decision tree for a max depth 2\n",
        "                         graph = Source( tree.export_graphviz(id3_model, out_file = None))\n",
        "                        SVG(graph.pipe(format='svg'))\n",
        "\n",
        "\n",
        "       Check whether root node that you identified is same as the one returned by Scikit DecisionTreeClassifier."
      ],
      "metadata": {
        "id": "8dFKvhmEAOQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load the dataset\n",
        "tennis_data = pd.read_csv('play (1).csv')\n",
        "\n",
        "# separate features and target\n",
        "X = tennis_data.drop(['play'], axis=1)\n",
        "y = tennis_data['play']\n",
        "\n",
        "# convert categorical variables to numerical\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# initialize the Decision Tree Classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# fit the model on the training data\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the testing data\n",
        "y_pred = dtc.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "accuracy = dtc.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1rlCOCZANZ_",
        "outputId": "9240449e-cd74-465e-f6e1-c63c5c5cc9ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log2\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import graphviz\n",
        "from IPython.display import SVG\n",
        "from graphviz import Source\n",
        "from sklearn import tree\n",
        "\n",
        "# Load the PlayTennis dataset\n",
        "data = pd.read_csv(\"play (1).csv\")\n",
        "\n",
        "# Label encoding for string columns\n",
        "le = LabelEncoder()\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "def entropy(class_col):\n",
        "    \"\"\"Function to calculate entropy of a given column\"\"\"\n",
        "    values, counts = np.unique(class_col, return_counts=True)\n",
        "    probs = counts / len(class_col)\n",
        "    entropy = sum([-p*np.log2(p) for p in probs])\n",
        "    return entropy\n",
        "\n",
        "def info_gain(data, split_attribute_name, target_attribute_name, trace=0):\n",
        "    \"\"\"Function to calculate information gain of a given attribute\"\"\"\n",
        "    \n",
        "    total_entropy = entropy(data[target_attribute_name])\n",
        "    \n",
        "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "    \n",
        "\n",
        "    weighted_entropy = sum([(counts[i]/sum(counts))*\n",
        "                            entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_attribute_name])\n",
        "                            for i in range(len(vals))])\n",
        "    \n",
        "\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "    \n",
        "    if trace:\n",
        "        print(\"Information gain of {}: {:.4f}\".format(split_attribute_name, info_gain))\n",
        "    \n",
        "    return info_gain\n",
        "\n",
        "\n",
        "for col in data.columns[:-1]:\n",
        "    info_gain(data, col, \"play\", trace=1)\n",
        "\n",
        "\n",
        "X_train = data.drop(['play','day'], axis=1)\n",
        "y_train = data['play']\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "test_sample = [[1, 0, 1, 1]]  # Rain, Cool, High, Weak\n",
        "\n",
        "# Train the decision tree classifier\n",
        "dtc = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "# Classify the test sample\n",
        "y_pred = dtc.predict(test_sample)\n",
        "\n",
        "# Print the predicted class of the test sample\n",
        "print(\"Predicted class: {}\".format(y_pred[0]))\n",
        "\n",
        "# Print the decision tree with max depth 2\n",
        "dot_data = tree.export_graphviz(dtc, out_file=None, feature_names=X_train.columns, class_names=['No', 'Yes'], filled=True, rounded=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "SVG(graph.pipe(format='svg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CNpCQi6JA4gH",
        "outputId": "64d44e28-08d5-4292-a422-d28c770fd308"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information gain of day: 0.9403\n",
            "Information gain of outlook: 0.2467\n",
            "Information gain of temp: 0.0292\n",
            "Information gain of humidity: 0.1518\n",
            "Information gain of wind: 0.0481\n",
            "    outlook  temp  humidity  wind\n",
            "0         2     1         0     1\n",
            "1         2     1         0     0\n",
            "2         0     1         0     1\n",
            "3         1     2         0     1\n",
            "4         1     0         1     1\n",
            "5         1     0         1     0\n",
            "6         0     0         1     0\n",
            "7         2     2         0     1\n",
            "8         2     0         1     1\n",
            "9         1     2         1     1\n",
            "10        2     2         1     0\n",
            "11        0     2         0     0\n",
            "12        0     1         1     1\n",
            "13        1     2         0     0\n",
            "0     0\n",
            "1     0\n",
            "2     1\n",
            "3     1\n",
            "4     1\n",
            "5     0\n",
            "6     1\n",
            "7     0\n",
            "8     1\n",
            "9     1\n",
            "10    1\n",
            "11    1\n",
            "12    1\n",
            "13    0\n",
            "Name: play, dtype: int64\n",
            "Predicted class: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"280pt\" height=\"314pt\" viewBox=\"0.00 0.00 280.00 314.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 276,-310 276,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#a7d3f3\" stroke=\"black\" d=\"M147,-306C147,-306 68,-306 68,-306 62,-306 56,-300 56,-294 56,-294 56,-235 56,-235 56,-229 62,-223 68,-223 68,-223 147,-223 147,-223 153,-223 159,-229 159,-235 159,-235 159,-294 159,-294 159,-300 153,-306 147,-306\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">outlook &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.459</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 14</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 9]</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Yes</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#399de5\" stroke=\"black\" d=\"M83,-179.5C83,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 83,-111.5 83,-111.5 89,-111.5 95,-117.5 95,-123.5 95,-123.5 95,-167.5 95,-167.5 95,-173.5 89,-179.5 83,-179.5\"/>\n<text text-anchor=\"middle\" x=\"47.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"47.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n<text text-anchor=\"middle\" x=\"47.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4]</text>\n<text text-anchor=\"middle\" x=\"47.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Yes</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0-&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M86.68,-222.91C81.02,-211.87 74.88,-199.9 69.18,-188.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"72.19,-186.97 64.51,-179.67 65.96,-190.16 72.19,-186.97\"/>\n<text text-anchor=\"middle\" x=\"56.85\" y=\"-199.76\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M211.5,-187C211.5,-187 125.5,-187 125.5,-187 119.5,-187 113.5,-181 113.5,-175 113.5,-175 113.5,-116 113.5,-116 113.5,-110 119.5,-104 125.5,-104 125.5,-104 211.5,-104 211.5,-104 217.5,-104 223.5,-110 223.5,-116 223.5,-116 223.5,-175 223.5,-175 223.5,-181 217.5,-187 211.5,-187\"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">humidity &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 5]</text>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = No</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M128.66,-222.91C133.21,-214.2 138.05,-204.9 142.75,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"145.85,-197.51 147.37,-187.02 139.65,-194.27 145.85,-197.51\"/>\n<text text-anchor=\"middle\" x=\"154.88\" y=\"-207.17\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#eca06a\" stroke=\"black\" d=\"M147,-68C147,-68 76,-68 76,-68 70,-68 64,-62 64,-56 64,-56 64,-12 64,-12 64,-6 70,0 76,0 76,0 147,0 147,0 153,0 159,-6 159,-12 159,-12 159,-56 159,-56 159,-62 153,-68 147,-68\"/>\n<text text-anchor=\"middle\" x=\"111.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.32</text>\n<text text-anchor=\"middle\" x=\"111.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n<text text-anchor=\"middle\" x=\"111.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 1]</text>\n<text text-anchor=\"middle\" x=\"111.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = No</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2-&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M147.28,-103.73C142.76,-95.06 137.99,-85.9 133.46,-77.18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"136.56,-75.55 128.83,-68.3 130.35,-78.79 136.56,-75.55\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#6ab6ec\" stroke=\"black\" d=\"M260,-68C260,-68 189,-68 189,-68 183,-68 177,-62 177,-56 177,-56 177,-12 177,-12 177,-6 183,0 189,0 189,0 260,0 260,0 266,0 272,-6 272,-12 272,-12 272,-56 272,-56 272,-62 266,-68 260,-68\"/>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.32</text>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 4]</text>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Yes</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2-&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M189.35,-103.73C193.74,-95.15 198.37,-86.09 202.79,-77.46\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"206.03,-78.8 207.47,-68.3 199.8,-75.61 206.03,-78.8\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Predict heart disease\n",
        "\n",
        "\n",
        "a.\tApply decision tree classifier on heart disease dataset and report the accuracy.\n",
        "\n",
        "b.\tTry to change the following hyperparameters of the decision tree model and report the change in the accuracy\n",
        "\n",
        "                    i.\tSplitting Criterion from entropy to gini\n",
        "                    ii.\tmax_depth \n",
        "                    iii.\tmin_samples_leaf\n"
      ],
      "metadata": {
        "id": "l5DHXm43BEkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "# Separate X (features) and y (target)\n",
        "X = df.drop(columns=['TenYearCHD'])\n",
        "y = df['TenYearCHD']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer()\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='entropy')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=5)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "clf = DecisionTreeClassifier(min_samples_leaf=10)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY_vIw3dBPFZ",
        "outputId": "59125483-9fd9-4b5d-f2d5-b30434242104"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7346698113207547\n",
            "Accuracy: 0.7712264150943396\n",
            "Accuracy: 0.8466981132075472\n",
            "Accuracy: 0.8254716981132075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create initial decision tree model\n",
        "model = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure accuracy of initial model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Initial Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Change splitting criterion from entropy to gini\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure accuracy of model with gini criterion\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Gini Criterion Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Change max depth of decision tree\n",
        "model = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure accuracy of model with limited max depth\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Max Depth 3 Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Change min samples leaf of decision tree\n",
        "model = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=5, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure accuracy of model with minimum samples per leaf\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Min Samples Leaf 5 Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb-ZcnlXBiML",
        "outputId": "597e894a-7be6-4075-ae62-f05bc8301a37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Model Accuracy: 1.00\n",
            "Gini Criterion Model Accuracy: 1.00\n",
            "Max Depth 3 Model Accuracy: 1.00\n",
            "Min Samples Leaf 5 Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRpqHfJK9-iF",
        "outputId": "27e29627-3e84-4423-fda1-96e718019185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Gains:\n",
            "day : 0.9402859586706311\n",
            "outlook : 0.24674981977443933\n",
            "temp : 0.02922256565895487\n",
            "humidity : 0.15183550136234159\n",
            "wind : 0.04812703040826949\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log2\n",
        "\n",
        "df = pd.read_csv('play (1).csv')\n",
        "\n",
        "def entropy(values):\n",
        "    freq = {}\n",
        "    for val in values:\n",
        "        if val in freq:\n",
        "            freq[val] += 1\n",
        "        else:\n",
        "            freq[val] = 1\n",
        "    entropy = 0\n",
        "    total = len(values)\n",
        "    for val, count in freq.items():\n",
        "        p = count/total\n",
        "        entropy -= p*log2(p)\n",
        "    return entropy\n",
        "\n",
        "target_entropy = entropy(df['play'])\n",
        "\n",
        "info_gains = {}\n",
        "for col in df.columns[:-1]:\n",
        "    values = df[col].unique()\n",
        "    subsets = []\n",
        "    for val in values:\n",
        "        subsets.append(df[df[col] == val]['play'])\n",
        "    subset_entropies = [entropy(subset) for subset in subsets]\n",
        "    weights = [len(subset)/len(df) for subset in subsets]\n",
        "    info_gain = target_entropy - sum([w*subset_entropy for w, subset_entropy in zip(weights, subset_entropies)])\n",
        "    info_gains[col] = info_gain\n",
        "\n",
        "print(\"Information Gains:\")\n",
        "for col, ig in info_gains.items():\n",
        "    print(col, \":\", ig)\n",
        "\n"
      ]
    }
  ]
}